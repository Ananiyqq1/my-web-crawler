# ğŸ•·ï¸ Web Crawler

[![CI](https://github.com/Ananiyqq1/my-web-crawler/actions/workflows/ci.yml/badge.svg)](https://github.com/Ananiyqq1/my-web-crawler/actions/workflows/ci.yml)
[![Go](https://img.shields.io/badge/Go-1.23-00ADD8?logo=go&logoColor=white)](https://go.dev)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-4169E1?logo=postgresql&logoColor=white)](https://www.postgresql.org)
[![Redis](https://img.shields.io/badge/Redis-7-DC382D?logo=redis&logoColor=white)](https://redis.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A high-performance, distributed web crawler built with **Go** for the crawling engine and **FastAPI** (Python) for the management REST API. Designed to be polite, concurrent, and scalable.

---

## âœ¨ Features

- ğŸš€ **Concurrent crawling** with configurable worker pools (Go goroutines)
- ğŸ¤– **robots.txt compliance** â€” respects directives and crawl-delay
- â±ï¸ **Per-domain rate limiting** via token bucket algorithm
- ğŸ”— **Link extraction & URL normalization** with depth-limited BFS
- ğŸ“¡ **REST API** (FastAPI) to start, stop, and monitor crawl jobs
- ğŸ“Š **WebSocket live feed** for real-time crawl progress
- ğŸ—„ï¸ **PostgreSQL** for persistent storage of jobs and pages
- ğŸ“¨ **Redis** message queue for decoupled job distribution
- ğŸ³ **Dockerized** â€” one command to run everything
- ğŸ“ˆ **Prometheus + Grafana** metrics and dashboards

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚       â”‚             â”‚       â”‚                  â”‚
â”‚   Client /   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  FastAPI    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚     Redis        â”‚
â”‚   Dashboard  â”‚â—€â”€â”€â”€â”€â”€â”€â”‚  (Python)   â”‚       â”‚  (Job Queue)     â”‚
â”‚              â”‚  WS   â”‚             â”‚       â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                       â”‚
                              â”‚                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚             â”‚       â”‚                  â”‚
                       â”‚ PostgreSQL  â”‚â—€â”€â”€â”€â”€â”€â”€â”‚   Go Crawler     â”‚
                       â”‚  (Storage)  â”‚       â”‚   (Workers)      â”‚
                       â”‚             â”‚       â”‚                  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Project Structure

```
my-web-crawler/
â”œâ”€â”€ crawler/                 # Go crawling engine
â”‚   â”œâ”€â”€ cmd/                 # CLI entry points
â”‚   â”‚   â”œâ”€â”€ crawler/         # Standalone crawler CLI
â”‚   â”‚   â””â”€â”€ worker/          # Redis consumer worker
â”‚   â”œâ”€â”€ internal/            # Private packages
â”‚   â”‚   â”œâ”€â”€ fetcher/         # HTTP client
â”‚   â”‚   â”œâ”€â”€ parser/          # HTML parsing & link extraction
â”‚   â”‚   â”œâ”€â”€ robots/          # robots.txt handler
â”‚   â”‚   â”œâ”€â”€ ratelimit/       # Per-domain rate limiter
â”‚   â”‚   â”œâ”€â”€ worker/          # Goroutine worker pool
â”‚   â”‚   â”œâ”€â”€ storage/         # Database layer (pgx)
â”‚   â”‚   â””â”€â”€ queue/           # Redis queue consumer
â”‚   â”œâ”€â”€ migrations/          # Go database migrations
â”‚   â”œâ”€â”€ go.mod
â”‚   â””â”€â”€ go.sum
â”œâ”€â”€ api/                     # FastAPI management service
â”‚   â”œâ”€â”€ main.py              # App entry point
â”‚   â”œâ”€â”€ routers/             # Route handlers
â”‚   â”œâ”€â”€ models/              # Pydantic schemas
â”‚   â”œâ”€â”€ db/                  # SQLAlchemy models & CRUD
â”‚   â”œâ”€â”€ services/            # Business logic & queue
â”‚   â”œâ”€â”€ tests/               # pytest test suite
â”‚   â”œâ”€â”€ alembic/             # Database migrations
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ infra/                   # Infrastructure configs
â”‚   â”œâ”€â”€ grafana/             # Grafana dashboard JSON
â”‚   â”œâ”€â”€ prometheus/          # Prometheus config
â”‚   â””â”€â”€ docker-compose.yml   # Local dev stack
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ data-models.md
â”‚   â”œâ”€â”€ tech-stack.md
â”‚   â”œâ”€â”€ api-spec.yaml
â”‚   â”œâ”€â”€ setup.md
â”‚   â””â”€â”€ deployment.md
â”œâ”€â”€ scripts/                 # Helper scripts
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/      # Issue templates
â”‚   â””â”€â”€ workflows/           # CI/CD pipelines
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) & [Docker Compose](https://docs.docker.com/compose/)
- [Go 1.23+](https://go.dev/dl/) (for local crawler development)
- [Python 3.11+](https://www.python.org/downloads/) (for local API development)

### Run Everything with Docker

```bash
# Clone the repo
git clone https://github.com/Ananiyqq1/my-web-crawler.git
cd my-web-crawler

# Start all services
docker compose -f infra/docker-compose.yml up -d

# API is now available at http://localhost:8000
# Swagger docs at http://localhost:8000/docs
```

### Start a Crawl Job

```bash
# Create a new crawl job
curl -X POST http://localhost:8000/crawl \
  -H "Content-Type: application/json" \
  -d '{"seed_urls": ["https://example.com"], "max_depth": 2}'

# Check job status
curl http://localhost:8000/crawl/{job_id}

# List crawled pages
curl http://localhost:8000/crawl/{job_id}/pages
```

---

## ğŸ§ª Running Tests

### Go Crawler
```bash
cd crawler
go test ./... -race -cover
```

### FastAPI
```bash
cd api
pip install -r requirements.txt
pytest tests/ -v --cov
```

---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/crawl` | Start a new crawl job |
| `GET` | `/crawl/{job_id}` | Get job status |
| `GET` | `/crawl/{job_id}/pages` | List crawled pages (paginated) |
| `GET` | `/pages/{page_id}` | Get specific page details |
| `DELETE` | `/crawl/{job_id}` | Stop/cancel a job |
| `WS` | `/crawl/{job_id}/stream` | Live progress feed |
| `GET` | `/health` | Health check |

Full OpenAPI spec: [docs/api-spec.yaml](docs/api-spec.yaml)

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Crawler Engine | Go 1.23, goquery, temoto/robotstxt |
| Management API | Python 3.11, FastAPI, Pydantic v2 |
| Database | PostgreSQL 16 |
| Message Queue | Redis 7 |
| Migrations | golang-migrate (Go), Alembic (Python) |
| Containers | Docker, Docker Compose |
| CI/CD | GitHub Actions |
| Monitoring | Prometheus, Grafana |

---

## ğŸ¤ Contributing

See [docs/contributing.md](docs/contributing.md) for guidelines.

1. Fork the repository
2. Create a feature branch: `git checkout -b feat/your-feature`
3. Commit your changes: `git commit -m "feat: add your feature"`
4. Push to the branch: `git push origin feat/your-feature`
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Ananiya H/meskel** â€” [@Ananiyqq1](https://github.com/Ananiyqq1)
